{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6202ee7-de6e-4ea1-a65d-95c9542002eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28564 files belonging to 206 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    r\"C:\\Users\\ashua\\Downloads\\birdclef-2025\\train_spectrograms\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",  \n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c904dca-4666-4b58-ae14-33d0e7a53eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CLASS_NAMES = sorted(os.listdir(\"train_audio\"))\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03a2e27-70e1-4987-a318-806b1c59a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = dataset.take(50)\n",
    "train_ds = dataset.skip(50)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bc8df5-1b27-4067-a43a-72eebe8f3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 1s/step - accuracy: 0.0240 - loss: 34.0291 - val_accuracy: 0.0394 - val_loss: 5.7137 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m901s\u001b[0m 1s/step - accuracy: 0.0341 - loss: 5.6167 - val_accuracy: 0.0406 - val_loss: 5.3528 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m909s\u001b[0m 1s/step - accuracy: 0.0340 - loss: 5.2934 - val_accuracy: 0.0413 - val_loss: 5.1321 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 1s/step - accuracy: 0.0345 - loss: 5.0888 - val_accuracy: 0.0400 - val_loss: 4.9782 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m887s\u001b[0m 1s/step - accuracy: 0.0337 - loss: 4.9530 - val_accuracy: 0.0388 - val_loss: 4.8825 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m917s\u001b[0m 1s/step - accuracy: 0.0345 - loss: 4.8619 - val_accuracy: 0.0394 - val_loss: 4.8154 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 1s/step - accuracy: 0.0346 - loss: 4.8074 - val_accuracy: 0.0400 - val_loss: 4.7714 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m867s\u001b[0m 1s/step - accuracy: 0.0335 - loss: 5.0325 - val_accuracy: 0.0369 - val_loss: 4.8917 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m867s\u001b[0m 1s/step - accuracy: 0.0338 - loss: 4.8683 - val_accuracy: 0.0400 - val_loss: 4.8240 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m875s\u001b[0m 1s/step - accuracy: 0.0341 - loss: 4.8104 - val_accuracy: 0.0406 - val_loss: 4.7899 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m869s\u001b[0m 1s/step - accuracy: 0.0341 - loss: 4.7868 - val_accuracy: 0.0388 - val_loss: 4.7805 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 1s/step - accuracy: 0.0336 - loss: 4.7728 - val_accuracy: 0.0375 - val_loss: 4.7672 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m934s\u001b[0m 1s/step - accuracy: 0.0344 - loss: 4.7623 - val_accuracy: 0.0375 - val_loss: 4.7600 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 1s/step - accuracy: 0.0350 - loss: 4.7712 - val_accuracy: 0.0381 - val_loss: 4.7608 - learning_rate: 5.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m890s\u001b[0m 1s/step - accuracy: 0.0330 - loss: 4.7606 - val_accuracy: 0.0400 - val_loss: 4.7352 - learning_rate: 5.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m920s\u001b[0m 1s/step - accuracy: 0.0344 - loss: 4.7479 - val_accuracy: 0.0400 - val_loss: 4.7295 - learning_rate: 5.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 1s/step - accuracy: 0.0346 - loss: 4.7387 - val_accuracy: 0.0381 - val_loss: 4.7313 - learning_rate: 5.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 1s/step - accuracy: 0.0337 - loss: 4.7370 - val_accuracy: 0.0419 - val_loss: 4.7297 - learning_rate: 5.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m921s\u001b[0m 1s/step - accuracy: 0.0336 - loss: 4.7300 - val_accuracy: 0.0400 - val_loss: 4.7276 - learning_rate: 5.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m843/843\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m923s\u001b[0m 1s/step - accuracy: 0.0335 - loss: 4.7325 - val_accuracy: 0.0413 - val_loss: 4.7171 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3),\n",
    "           kernel_regularizer=regularizers.l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu',\n",
    "           kernel_regularizer=regularizers.l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu',\n",
    "          kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, lr_reduce, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7133d4-afcc-4728-91c4-64118fe66df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"birdclef_cnn_model.keras\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74110aa5-f7cc-41bf-959c-1ec601c159d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"birdclef_cnn_model.keras\")\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7084b94-2ff9-447f-a346-a4b1a8a85548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "✅ Predicted Bird Species: grekis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "AUDIO_PATH = r\"C:\\Users\\ashua\\Downloads\\birdclef-2025\\train_audio\\wbwwre1\\iNat45372.ogg\" \n",
    "TEMP_IMG_PATH = \"temp_test.png\"\n",
    "MODEL_PATH = \"birdclef_cnn_model.keras\"\n",
    "CLASS_NAMES = sorted(os.listdir(r\"C:\\Users\\ashua\\Downloads\\birdclef-2025\\train_spectrograms\")) \n",
    "\n",
    "\n",
    "\n",
    "def audio_to_image(audio_path, out_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None, duration=60.0)\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(2.24, 2.24))\n",
    "        ax.axis('off')\n",
    "        librosa.display.specshow(S_DB, sr=sr, ax=ax)\n",
    "        fig.savefig(out_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        plt.close(fig)\n",
    "        return out_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to generate spectrogram: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "def predict_from_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)\n",
    "    idx = np.argmax(pred)\n",
    "    return CLASS_NAMES[idx]\n",
    "\n",
    "\n",
    "img_path = audio_to_image(AUDIO_PATH, TEMP_IMG_PATH)\n",
    "\n",
    "if img_path:\n",
    "    predicted_label = predict_from_image(img_path)\n",
    "    print(f\"✅ Predicted Bird Species: {predicted_label}\")\n",
    "else:\n",
    "    print(\"⚠️ Prediction skipped due to spectrogram failure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d14aba-3128-4393-90c8-38b88e9fc1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
